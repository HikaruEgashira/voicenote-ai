import { useState, useEffect, useRef, useCallback } from "react";
import {
  Text,
  View,
  TouchableOpacity,
  TextInput,
  StyleSheet,
  Platform,
  Alert,
  Animated,
  ScrollView,
} from "react-native";
import { useRouter } from "expo-router";
import { useAudioRecorder, RecordingPresets, AudioModule, setAudioModeAsync } from "expo-audio";
import { useKeepAwake } from "expo-keep-awake";
import * as FileSystem from "expo-file-system/legacy";
import * as Haptics from "expo-haptics";
import AsyncStorage from "@react-native-async-storage/async-storage";

import { ScreenContainer } from "@/components/screen-container";
import { IconSymbol } from "@/components/ui/icon-symbol";
import { useRecordings } from "@/lib/recordings-context";
import { useColors } from "@/hooks/use-colors";
import { Recording, Highlight } from "@/types/recording";
import { useRealtimeTranscription } from "@/hooks/use-realtime-transcription";

const RECORDING_OPTIONS = RecordingPresets.HIGH_QUALITY;

function formatTime(seconds: number): string {
  const mins = Math.floor(seconds / 60);
  const secs = Math.floor(seconds % 60);
  const ms = Math.floor((seconds % 1) * 100);
  return `${mins.toString().padStart(2, "0")}:${secs.toString().padStart(2, "0")}.${ms.toString().padStart(2, "0")}`;
}

export default function RecordScreen() {
  const router = useRouter();
  const colors = useColors();
  const { addRecording, updateRealtimeTranscript, clearRealtimeTranscript } = useRecordings();

  const [isRecording, setIsRecording] = useState(false);
  const [isPaused, setIsPaused] = useState(false);
  const [duration, setDuration] = useState(0);
  const [highlights, setHighlights] = useState<Highlight[]>([]);
  const [notes, setNotes] = useState("");
  const [hasPermission, setHasPermission] = useState<boolean | null>(null);
  const [realtimeEnabled, setRealtimeEnabled] = useState(false);
  const [currentRecordingId, setCurrentRecordingId] = useState<string | null>(null);

  const audioRecorder = useAudioRecorder(RECORDING_OPTIONS);
  const timerRef = useRef<ReturnType<typeof setInterval> | null>(null);
  const pulseAnim = useRef(new Animated.Value(1)).current;

  // Realtime transcription hook
  const {
    state: realtimeState,
    startSession: startRealtimeSession,
    stopSession: stopRealtimeSession,
  } = useRealtimeTranscription();

  // Keep screen awake during recording
  useKeepAwake();

  // Load settings
  useEffect(() => {
    const loadSettings = async () => {
      try {
        const saved = await AsyncStorage.getItem("app-settings");
        if (saved) {
          const settings = JSON.parse(saved);
          setRealtimeEnabled(settings.realtimeTranscription?.enabled || false);
        }
      } catch (error) {
        console.error("Failed to load settings:", error);
      }
    };
    loadSettings();
  }, []);

  // Request microphone permission and set audio mode for background recording
  useEffect(() => {
    (async () => {
      // Set audio mode for background recording
      await setAudioModeAsync({
        playsInSilentMode: true,
        allowsRecording: true,
        shouldPlayInBackground: true,
        allowsBackgroundRecording: true,
      });
      
      const status = await AudioModule.requestRecordingPermissionsAsync();
      setHasPermission(status.granted);
    })();
  }, []);

  // Timer for duration
  useEffect(() => {
    if (isRecording && !isPaused) {
      timerRef.current = setInterval(() => {
        setDuration((prev) => prev + 0.1);
      }, 100);
    } else {
      if (timerRef.current) {
        clearInterval(timerRef.current);
        timerRef.current = null;
      }
    }

    return () => {
      if (timerRef.current) {
        clearInterval(timerRef.current);
      }
    };
  }, [isRecording, isPaused]);

  // Pulse animation for recording indicator
  useEffect(() => {
    if (isRecording && !isPaused) {
      const pulse = Animated.loop(
        Animated.sequence([
          Animated.timing(pulseAnim, {
            toValue: 1.2,
            duration: 800,
            useNativeDriver: true,
          }),
          Animated.timing(pulseAnim, {
            toValue: 1,
            duration: 800,
            useNativeDriver: true,
          }),
        ])
      );
      pulse.start();
      return () => pulse.stop();
    } else {
      pulseAnim.setValue(1);
    }
  }, [isRecording, isPaused, pulseAnim]);

  // Sync realtime transcript to context
  useEffect(() => {
    if (currentRecordingId && realtimeState.segments.length > 0) {
      updateRealtimeTranscript(currentRecordingId, realtimeState.segments);
    }
  }, [currentRecordingId, realtimeState.segments, updateRealtimeTranscript]);

  const handleStartRecording = useCallback(async () => {
    if (Platform.OS !== "web") {
      Haptics.impactAsync(Haptics.ImpactFeedbackStyle.Medium);
    }

    try {
      // Prepare the recorder before starting
      await audioRecorder.prepareToRecordAsync();
      audioRecorder.record();
      setIsRecording(true);
      setIsPaused(false);
      setDuration(0);
      setHighlights([]);
      setNotes("");

      // Create temporary recording ID for realtime transcription
      const recordingId = Date.now().toString();
      setCurrentRecordingId(recordingId);

      // Start realtime transcription if enabled
      if (realtimeEnabled) {
        try {
          await startRealtimeSession(recordingId);
          console.log("Realtime transcription session started");
        } catch (error) {
          console.error("Failed to start realtime session:", error);
          // Continue recording even if realtime fails
        }
      }
    } catch (error) {
      console.error("Failed to start recording:", error);
      Alert.alert("エラー", "録音を開始できませんでした");
    }
  }, [audioRecorder, realtimeEnabled, startRealtimeSession]);

  const handlePauseResume = useCallback(async () => {
    if (Platform.OS !== "web") {
      Haptics.impactAsync(Haptics.ImpactFeedbackStyle.Light);
    }

    try {
      if (isPaused) {
        await audioRecorder.record();
        setIsPaused(false);
      } else {
        audioRecorder.pause();
        setIsPaused(true);
      }
    } catch (error) {
      console.error("Failed to pause/resume:", error);
    }
  }, [audioRecorder, isPaused]);

  const handleStopRecording = useCallback(async () => {
    if (Platform.OS !== "web") {
      Haptics.notificationAsync(Haptics.NotificationFeedbackType.Success);
    }

    try {
      // Stop realtime transcription session
      if (realtimeEnabled && currentRecordingId) {
        try {
          await stopRealtimeSession();
          console.log("Realtime transcription session stopped");
        } catch (error) {
          console.error("Failed to stop realtime session:", error);
        }
      }

      await audioRecorder.stop();
      setIsRecording(false);
      setIsPaused(false);

      const uri = audioRecorder.uri;
      if (!uri) {
        Alert.alert("エラー", "録音ファイルが見つかりません");
        return;
      }

      console.log("Recording stopped, URI:", uri);

      let finalUri = uri;
      let audioBase64: string | undefined;

      // On Web, convert blob to base64 and store it
      // Blob URLs are temporary and won't work after page reload
      if (Platform.OS === "web") {
        console.log("Web platform - converting blob to base64 for storage");
        try {
          const response = await fetch(uri);
          const blob = await response.blob();

          // Convert blob to base64
          const base64Data = await new Promise<string>((resolve, reject) => {
            const reader = new FileReader();
            reader.onloadend = () => {
              const result = reader.result as string;
              // Remove data URL prefix (e.g., "data:audio/webm;base64,")
              const base64 = result.split(',')[1];
              resolve(base64);
            };
            reader.onerror = reject;
            reader.readAsDataURL(blob);
          });

          console.log("Base64 conversion completed, length:", base64Data.length);
          audioBase64 = base64Data;
          // Use data URL for web storage
          finalUri = `data:audio/webm;base64,${base64Data}`;
        } catch (webError) {
          console.error("Failed to convert blob to base64:", webError);
          throw new Error("録音データの変換に失敗しました");
        }
      } else {
        // Native platforms: move file to permanent location
        // Generate unique filename
        const timestamp = new Date().toISOString().replace(/[:.]/g, "-");
        const filename = `recording_${timestamp}.m4a`;
        const newUri = `${FileSystem.documentDirectory}recordings/${filename}`;

        // Ensure directory exists
        await FileSystem.makeDirectoryAsync(`${FileSystem.documentDirectory}recordings/`, {
          intermediates: true,
        });

        // Move file to permanent location
        await FileSystem.moveAsync({
          from: uri,
          to: newUri,
        });

        finalUri = newUri;
      }

      // Create recording object
      const now = new Date();
      const recordingId = currentRecordingId || Date.now().toString();
      const newRecording: Recording = {
        id: recordingId,
        title: `録音 ${now.toLocaleDateString("ja-JP")} ${now.toLocaleTimeString("ja-JP", { hour: "2-digit", minute: "2-digit" })}`,
        audioUri: finalUri,
        duration: Math.floor(duration),
        createdAt: now,
        updatedAt: now,
        highlights,
        notes,
        qaHistory: [],
        status: "saved",
      };

      console.log("Adding recording:", newRecording.id);
      await addRecording(newRecording);
      console.log("Recording added successfully");

      // Reset state
      setDuration(0);
      setHighlights([]);
      setNotes("");
      setCurrentRecordingId(null);

      // Navigate to note detail
      router.push(`/note/${newRecording.id}`);
    } catch (error) {
      console.error("Failed to stop recording:", error);
      Alert.alert("エラー", "録音の保存に失敗しました");
    }
  }, [audioRecorder, duration, highlights, notes, addRecording, router, currentRecordingId, realtimeEnabled, stopRealtimeSession]);

  const handleAddHighlight = useCallback(() => {
    if (Platform.OS !== "web") {
      Haptics.impactAsync(Haptics.ImpactFeedbackStyle.Heavy);
    }

    const newHighlight: Highlight = {
      id: Date.now().toString(),
      timestamp: duration,
    };
    setHighlights((prev) => [...prev, newHighlight]);
  }, [duration]);

  if (hasPermission === null) {
    return (
      <ScreenContainer>
        <View style={styles.centered}>
          <Text style={{ color: colors.foreground }}>マイクの許可を確認中...</Text>
        </View>
      </ScreenContainer>
    );
  }

  if (hasPermission === false) {
    return (
      <ScreenContainer>
        <View style={styles.centered}>
          <IconSymbol name="mic.fill" size={64} color={colors.error} />
          <Text style={[styles.permissionTitle, { color: colors.foreground }]}>
            マイクへのアクセスが必要です
          </Text>
          <Text style={[styles.permissionText, { color: colors.muted }]}>
            設定からマイクへのアクセスを許可してください
          </Text>
        </View>
      </ScreenContainer>
    );
  }

  return (
    <ScreenContainer>
      <View style={styles.container}>
        {/* Header */}
        <View style={styles.header}>
          <Text style={[styles.title, { color: colors.foreground }]}>
            {isRecording ? "録音中" : "新規録音"}
          </Text>
          {isRecording && (
            <View style={styles.recordingIndicator}>
              <Animated.View
                style={[
                  styles.recordingDot,
                  { backgroundColor: colors.recording, transform: [{ scale: pulseAnim }] },
                ]}
              />
              <Text style={[styles.recordingText, { color: colors.recording }]}>REC</Text>
            </View>
          )}
        </View>

        {/* Timer */}
        <View style={styles.timerContainer}>
          <Text style={[styles.timer, { color: colors.foreground }]}>{formatTime(duration)}</Text>
        </View>

        {/* Waveform placeholder */}
        <View style={[styles.waveform, { backgroundColor: colors.surface }]}>
          <View style={styles.waveformBars}>
            {Array.from({ length: 50 }).map((_, i) => (
              <View
                key={i}
                style={[
                  styles.waveformBar,
                  {
                    backgroundColor: isRecording && !isPaused ? colors.primary : colors.border,
                    height: isRecording && !isPaused ? 10 + Math.random() * 50 : 20,
                  },
                ]}
              />
            ))}
          </View>
        </View>

        {/* Realtime Transcription */}
        {isRecording && realtimeEnabled && (
          <View style={[styles.realtimeSection, { backgroundColor: colors.surface, borderColor: colors.border }]}>
            <View style={styles.realtimeHeader}>
              <View style={styles.realtimeHeaderLeft}>
                <IconSymbol name="text.bubble" size={16} color={colors.primary} />
                <Text style={[styles.realtimeTitle, { color: colors.foreground }]}>
                  リアルタイム文字起こし
                </Text>
              </View>
              <View style={styles.realtimeStatus}>
                {realtimeState.connectionStatus === "connected" && (
                  <>
                    <View style={[styles.statusDot, { backgroundColor: colors.success }]} />
                    <Text style={[styles.statusText, { color: colors.success }]}>接続中</Text>
                  </>
                )}
                {realtimeState.connectionStatus === "connecting" && (
                  <>
                    <View style={[styles.statusDot, { backgroundColor: colors.warning }]} />
                    <Text style={[styles.statusText, { color: colors.warning }]}>接続中...</Text>
                  </>
                )}
                {realtimeState.connectionStatus === "error" && (
                  <>
                    <View style={[styles.statusDot, { backgroundColor: colors.error }]} />
                    <Text style={[styles.statusText, { color: colors.error }]}>エラー</Text>
                  </>
                )}
              </View>
            </View>
            <ScrollView
              style={styles.realtimeContent}
              showsVerticalScrollIndicator={false}
            >
              {realtimeState.segments.length === 0 ? (
                <Text style={[styles.realtimePlaceholder, { color: colors.muted }]}>
                  話し始めると、ここに文字起こし結果が表示されます...
                </Text>
              ) : (
                realtimeState.segments.map((segment) => (
                  <View key={segment.id} style={styles.segmentItem}>
                    {segment.speaker && (
                      <Text style={[styles.speakerLabel, { color: colors.secondary }]}>
                        [{segment.speaker}]
                      </Text>
                    )}
                    <Text
                      style={[
                        styles.segmentText,
                        {
                          color: segment.isPartial ? colors.muted : colors.foreground,
                          fontStyle: segment.isPartial ? "italic" : "normal",
                        },
                      ]}
                    >
                      {segment.text}
                    </Text>
                  </View>
                ))
              )}
            </ScrollView>
          </View>
        )}

        {/* Highlights count */}
        {highlights.length > 0 && (
          <View style={styles.highlightsInfo}>
            <IconSymbol name="star.fill" size={16} color={colors.highlight} />
            <Text style={[styles.highlightsText, { color: colors.highlight }]}>
              {highlights.length}個のハイライト
            </Text>
          </View>
        )}

        {/* Notes input */}
        {isRecording && (
          <View style={[styles.notesContainer, { backgroundColor: colors.surface, borderColor: colors.border }]}>
            <TextInput
              style={[styles.notesInput, { color: colors.foreground }]}
              placeholder="メモを追加..."
              placeholderTextColor={colors.muted}
              value={notes}
              onChangeText={setNotes}
              multiline
              maxLength={500}
            />
          </View>
        )}

        {/* Controls */}
        <View style={styles.controls}>
          {!isRecording ? (
            <TouchableOpacity
              onPress={handleStartRecording}
              style={[styles.recordButton, { backgroundColor: colors.primary }]}
              activeOpacity={0.8}
            >
              <IconSymbol name="mic.fill" size={40} color="#FFFFFF" />
            </TouchableOpacity>
          ) : (
            <View style={styles.recordingControls}>
              {/* Highlight button */}
              <TouchableOpacity
                onPress={handleAddHighlight}
                style={[styles.controlButton, { backgroundColor: colors.highlight }]}
              >
                <IconSymbol name="star.fill" size={24} color="#FFFFFF" />
              </TouchableOpacity>

              {/* Pause/Resume button */}
              <TouchableOpacity
                onPress={handlePauseResume}
                style={[styles.pauseButton, { backgroundColor: isPaused ? colors.primary : colors.surface }]}
              >
                <IconSymbol
                  name={isPaused ? "play.fill" : "pause.fill"}
                  size={32}
                  color={isPaused ? "#FFFFFF" : colors.foreground}
                />
              </TouchableOpacity>

              {/* Stop button */}
              <TouchableOpacity
                onPress={handleStopRecording}
                style={[styles.controlButton, { backgroundColor: colors.error }]}
              >
                <IconSymbol name="stop.fill" size={24} color="#FFFFFF" />
              </TouchableOpacity>
            </View>
          )}
        </View>

        {/* Instructions */}
        {!isRecording && (
          <Text style={[styles.instructions, { color: colors.muted }]}>
            ボタンをタップして録音を開始
          </Text>
        )}
      </View>
    </ScreenContainer>
  );
}

const styles = StyleSheet.create({
  container: {
    flex: 1,
    paddingHorizontal: 20,
  },
  centered: {
    flex: 1,
    alignItems: "center",
    justifyContent: "center",
    gap: 16,
  },
  permissionTitle: {
    fontSize: 18,
    fontWeight: "600",
    marginTop: 16,
  },
  permissionText: {
    fontSize: 14,
    textAlign: "center",
  },
  header: {
    flexDirection: "row",
    justifyContent: "space-between",
    alignItems: "center",
    paddingTop: 8,
    paddingBottom: 16,
  },
  title: {
    fontSize: 28,
    fontWeight: "700",
  },
  recordingIndicator: {
    flexDirection: "row",
    alignItems: "center",
    gap: 6,
  },
  recordingDot: {
    width: 10,
    height: 10,
    borderRadius: 5,
  },
  recordingText: {
    fontSize: 14,
    fontWeight: "700",
  },
  timerContainer: {
    alignItems: "center",
    paddingVertical: 32,
  },
  timer: {
    fontSize: 56,
    fontWeight: "300",
    fontVariant: ["tabular-nums"],
  },
  waveform: {
    height: 120,
    borderRadius: 16,
    padding: 16,
    justifyContent: "center",
  },
  waveformBars: {
    flexDirection: "row",
    alignItems: "center",
    justifyContent: "space-between",
    height: "100%",
  },
  waveformBar: {
    width: 3,
    borderRadius: 2,
  },
  highlightsInfo: {
    flexDirection: "row",
    alignItems: "center",
    justifyContent: "center",
    gap: 6,
    marginTop: 16,
  },
  highlightsText: {
    fontSize: 14,
    fontWeight: "500",
  },
  notesContainer: {
    marginTop: 20,
    borderRadius: 12,
    borderWidth: 1,
    padding: 12,
    minHeight: 80,
  },
  notesInput: {
    fontSize: 15,
    lineHeight: 22,
  },
  controls: {
    flex: 1,
    justifyContent: "center",
    alignItems: "center",
  },
  recordButton: {
    width: 96,
    height: 96,
    borderRadius: 48,
    alignItems: "center",
    justifyContent: "center",
    shadowColor: "#000",
    shadowOffset: { width: 0, height: 4 },
    shadowOpacity: 0.2,
    shadowRadius: 8,
    elevation: 8,
  },
  recordingControls: {
    flexDirection: "row",
    alignItems: "flex-end",
    gap: 24,
  },
  controlButton: {
    width: 56,
    height: 56,
    borderRadius: 28,
    alignItems: "center",
    justifyContent: "center",
  },
  pauseButton: {
    width: 80,
    height: 80,
    borderRadius: 40,
    alignItems: "center",
    justifyContent: "center",
  },
  instructions: {
    textAlign: "center",
    fontSize: 14,
    marginBottom: 40,
  },
  realtimeSection: {
    marginTop: 16,
    borderRadius: 12,
    borderWidth: 1,
    padding: 12,
    maxHeight: 200,
  },
  realtimeHeader: {
    flexDirection: "row",
    justifyContent: "space-between",
    alignItems: "center",
    marginBottom: 8,
  },
  realtimeHeaderLeft: {
    flexDirection: "row",
    alignItems: "center",
    gap: 6,
  },
  realtimeTitle: {
    fontSize: 14,
    fontWeight: "600",
  },
  realtimeStatus: {
    flexDirection: "row",
    alignItems: "center",
    gap: 4,
  },
  statusDot: {
    width: 6,
    height: 6,
    borderRadius: 3,
  },
  statusText: {
    fontSize: 12,
    fontWeight: "500",
  },
  realtimeContent: {
    maxHeight: 140,
  },
  realtimePlaceholder: {
    fontSize: 13,
    fontStyle: "italic",
    textAlign: "center",
    paddingVertical: 20,
  },
  segmentItem: {
    marginBottom: 8,
  },
  speakerLabel: {
    fontSize: 11,
    fontWeight: "600",
    marginBottom: 2,
  },
  segmentText: {
    fontSize: 14,
    lineHeight: 20,
  },
});
